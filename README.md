# ğŸŸ é­šé¡é«”é‡é æ¸¬ç³»çµ±ï¼ˆLassoï¼‹Optunaï¼‰â€”æŠ€è¡“å ±å‘Šï¼ˆMarkdownï¼Œç¹é«”ä¸­æ–‡ï¼‰

> æœ¬æ–‡ä»¶é‡å°ä½ æä¾›çš„ `Streamlit` æ‡‰ç”¨ç¨‹å¼é€²è¡Œ**é—œéµç¨‹å¼ç¢¼æ“·å–**èˆ‡**é€è¡Œèªªæ˜**ï¼Œä¸¦ä»¥ **CRISP-DM** æ­¥é©Ÿæ’°å¯«ã€‚æ–‡æœ«è£œå……å¸¸è¦‹éŒ¯èª¤èˆ‡ä¿®æ­£å»ºè­°ï¼ˆå«è³‡æ–™è·¯å¾‘å°é™·é˜±ï¼‰ã€‚
> Demo Link [https://aiot-hw2.streamlit.app/](https://aiot-hw2.streamlit.app/)
> GPT Prompt Link [https://chatgpt.com/share/68e5cf49-d8d0-8013-b6b4-984a12600050](https://chatgpt.com/share/68e5cf49-d8d0-8013-b6b4-984a12600050)
---

## ğŸ”— è³‡æ–™é›†é€£çµï¼ˆFish Market Datasetï¼‰

Kaggleï¼š[https://www.kaggle.com/datasets/salman1127/fish-market-dataset](https://www.kaggle.com/datasets/salman1127/fish-market-dataset)

> é€šå¸¸æª”åç‚º **`Fish.csv`**ã€‚è«‹ç•™æ„ä½ çš„ç¨‹å¼ä¸­çš„ `PATH` ç›®å‰å¯«æˆ `./dataset/Fishers maket.csv`ï¼ˆæœ‰æ‹¼å­—èˆ‡æª”åèª¤å·®ï¼‰ï¼Œå»ºè­°æ”¹ç‚ºï¼š`PATH = "./dataset/Fish.csv"`ã€‚

---

## ğŸ“˜ ä½œæ¥­æŒ‡å¼•ï¼ˆåŸæ¨£é‡ç¾ï¼‰

1. å†æ¬¡ä½¿ç”¨chatGPT å·¥å…·åˆ©ç”¨CRISP-DMæ¨¡æ¿è§£æ±º å¤šå…ƒå›æ­¸ Regression Problem
2. Step 1: çˆ¬èŸ²æŠ“å–Bostonæˆ¿åƒ¹
3. Step 2: Preprocessing : train test split
4. Step 3: Build Model using Lasso
5. Step 4: Evaluation: MSE, MAE, R2 metrics çš„æ„ç¾©, overfit and underfit çš„åˆ¤æ–·ï¼ˆç•«å‡º training, test curveï¼‰, å„ªåŒ–æ¨¡å‹ optuna
6. Step 5: Deployment

> æœ¬å°ˆæ¡ˆä»¥**é­šé¡è³‡æ–™é›†**æ›¿ä»£ Boston æˆ¿åƒ¹ï¼Œä»ä¾ CRISP-DM æµç¨‹å®Œæˆå¤šå…ƒè¿´æ­¸å°ˆé¡Œã€‚

---

# CRISP-DM æ¶æ§‹å°æ‡‰

* **Business & Data Understandingï¼ˆTab1ï¼‰ï¼š** è¼‰å…¥è³‡æ–™ã€çµ±è¨ˆæ‘˜è¦ã€ç›®æ¨™åˆ†ä½ˆ
* **Data Preparationï¼ˆTab2ï¼‰ï¼š** é¡åˆ¥ç·¨ç¢¼ã€è³‡æ–™åˆ†å‰²ã€æ¨™æº–åŒ–ã€ç›¸é—œæ€§åœ–
* **Modelingï¼ˆTab3ï¼‰ï¼š** Lasso è¿´æ­¸ï¼ˆæ‰‹å‹• Î± æˆ– Optuna æœå°‹ï¼‰
* **Evaluationï¼ˆTab4/Tab5ï¼‰ï¼š** è¨“ç·´/æ¸¬è©¦æŒ‡æ¨™ã€é/æ¬ æ“¬åˆåˆ¤æ–·ã€æ®˜å·®è¨ºæ–·ã€Optuna è¦–è¦ºåŒ–
* **Deploymentï¼ˆTab6ï¼‰ï¼š** å³æ™‚èˆ‡æ‰¹æ¬¡æ¨è«–ã€çµæœä¸‹è¼‰

---

## 0) åŸ·è¡Œæ–¹å¼

```bash
streamlit run app.py
```

---

## 1) åŒ¯å…¥èˆ‡å…¨åŸŸè¨­å®šï¼ˆé€è¡Œé‡é»ï¼‰

```python
import streamlit as st
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.linear_model import Lasso
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import matplotlib.pyplot as plt
import seaborn as sns
import optuna
from optuna.visualization.matplotlib import plot_optimization_history, plot_param_importances
import warnings
warnings.filterwarnings('ignore')
```

* `streamlit`ï¼šå‰ç«¯äº’å‹•å¼å„€è¡¨æ¿ã€‚
* `pandas/numpy`ï¼šè³‡æ–™è™•ç†ã€å‘é‡é‹ç®—ã€‚
* `sklearn`ï¼šè³‡æ–™åˆ†å‰²ã€ç‰¹å¾µæ¨™æº–åŒ–ã€Lasso æ¨¡å‹ã€è©•ä¼°æŒ‡æ¨™ã€äº¤å‰é©—è­‰ã€‚
* `matplotlib/seaborn`ï¼šç¹ªåœ–ã€‚
* `optuna`ï¼šè‡ªå‹•åŒ–è¶…åƒæ•¸å„ªåŒ–ï¼ˆé€™è£¡ç”¨ä¾†å°‹æ‰¾ **alpha**ï¼‰ã€‚
* é—œé–‰è­¦å‘Šè¨Šæ¯ä»¥ä¿æŒ UI ä¹¾æ·¨ã€‚

```python
plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'Microsoft JhengHei', 'SimHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False
```

* è¨­å®šä¸­æ–‡å­—å‹èˆ‡è² è™Ÿé¡¯ç¤ºï¼Œé¿å…äº‚ç¢¼/è² è™Ÿæ–¹å¡Šã€‚

```python
st.set_page_config(page_title="é­šé¡é«”é‡é æ¸¬ç³»çµ±", page_icon="ğŸŸ", layout="wide")
```

* è¨­å®šé é¢æ¨™é¡Œã€faviconã€å¯¬ç‰ˆç‰ˆé¢ã€‚

```python
PATH = "./dataset/Fishers maket.csv"
```

* **æ³¨æ„ï¼š** å»ºè­°æ”¹ç‚º `PATH = "./dataset/Fish.csv"`ï¼ˆKaggle å…¸å‹æª”åï¼‰ã€‚

---

## 2) å´é‚Šæ¬„åƒæ•¸ï¼ˆé€è¡Œé‡é»ï¼‰

```python
st.sidebar.header("ğŸ›ï¸ åƒæ•¸è¨­å®š")
st.sidebar.subheader("ğŸ“Š è³‡æ–™åˆ†å‰²åƒæ•¸")
test_size = st.sidebar.slider("æ¸¬è©¦é›†æ¯”ä¾‹", 0.1, 0.4, 0.2, 0.05)
random_state = st.sidebar.number_input("éš¨æ©Ÿç¨®å­", 1, 100, 42)

st.sidebar.subheader("ğŸ¤– æ¨¡å‹åƒæ•¸")
use_optuna = st.sidebar.checkbox("ä½¿ç”¨ Optuna å„ªåŒ–", value=False)

if not use_optuna:
    alpha = st.sidebar.slider("Alpha (æ­£å‰‡åŒ–å¼·åº¦)", 0.01, 10.0, 1.0, 0.1)
else:
    st.sidebar.info("âœ¨ Optuna å°‡è‡ªå‹•å°‹æ‰¾æœ€ä½³ Alpha å€¼")
    n_trials = st.sidebar.slider("Optuna è©¦é©—æ¬¡æ•¸", 10, 200, 50, 10)
```

* **äº’å‹•æ§åˆ¶**ï¼šå¯èª¿æ•´è³‡æ–™åˆ‡åˆ†æ¯”ä¾‹ã€éš¨æ©Ÿç¨®å­ï¼›é¸æ“‡æ‰‹å‹• Î± æˆ– Optuna æœå°‹ Î±ï¼›è¨­å®š Optuna è©¦é©—æ•¸ã€‚

---

## 3) è¼‰å…¥è³‡æ–™ï¼ˆcacheï¼‹ä¾‹å¤–è™•ç†ï¼‰

```python
@st.cache_data
def load_data(file_path):
    try:
        df = pd.read_csv(file_path)
        return df, None
    except Exception as e:
        return None, str(e)

df, error = load_data(PATH)

if error:
    st.error(f"âŒ {error}")
    st.stop()
```

* `@st.cache_data`ï¼šç›¸åŒè¼¸å…¥æ™‚å¿«å–çµæœï¼Œé¿å…é‡è¤‡è®€æª”ã€‚
* ä»¥ `try/except` å›å‚³éŒ¯èª¤å­—ä¸²ï¼›è‹¥å¤±æ•—ç›´æ¥çµ‚æ­¢æ‡‰ç”¨ï¼ˆé¡¯ç¤ºéŒ¯èª¤ï¼‰ã€‚

---

## 4) è³‡æ–™é è™•ç†ï¼ˆç·¨ç¢¼ã€åˆ‡åˆ†ã€æ¨™æº–åŒ–ï¼‰

```python
@st.cache_data
def preprocess_data(dataframe, test_sz, rand_state):
    X = dataframe.drop('Weight', axis=1)
    y = dataframe['Weight']
  
    if 'Species' in X.columns:
        X = pd.get_dummies(X, columns=['Species'], drop_first=True)
  
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=test_sz, random_state=rand_state
    )
  
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
  
    return X_train_scaled, X_test_scaled, y_train, y_test, scaler, X.columns

X_train, X_test, y_train, y_test, scaler, feature_names = preprocess_data(df, test_size, random_state)
```

* **ç›®æ¨™æ¬„ä½**ï¼š`Weight`
* **é¡åˆ¥ç·¨ç¢¼**ï¼š`Species` â†’ one-hotï¼ˆ`drop_first=True` é˜²å¤šé‡å…±ç·šæ€§ï¼‰
* **è³‡æ–™åˆ‡åˆ†**ï¼šè¨“ç·´/æ¸¬è©¦ï¼ˆä¿ç•™ `random_state` å¯é‡ç¾ï¼‰
* **æ¨™æº–åŒ–**ï¼š`StandardScaler`ï¼ˆä»¥è¨“ç·´é›† fitã€å¥—ç”¨è‡³æ¸¬è©¦é›†ï¼‰
* **å›å‚³**ï¼šç¸®æ”¾å¾Œè³‡æ–™ã€`scaler`ï¼ˆéƒ¨ç½²æ™‚è¦ä¸€è‡´è½‰æ›ï¼‰ã€`feature_names`ï¼ˆç•«åœ–/æ¨è«–æ’åˆ—å°é½Šï¼‰

---

## 5) Optuna ç›®æ¨™å‡½æ•¸èˆ‡å„ªåŒ–ï¼ˆäº¤å‰é©—è­‰ MSEï¼‰

```python
def objective(trial, X_tr, y_tr):
    alpha_param = trial.suggest_float('alpha', 0.001, 10.0, log=True)
    model = Lasso(alpha=alpha_param, random_state=42, max_iter=10000)
    scores = cross_val_score(model, X_tr, y_tr, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)
    return -scores.mean()
```

* **æœå°‹ç©ºé–“**ï¼š`alpha âˆˆ [0.001, 10]`ï¼Œlog å°ºåº¦ï¼ˆé©åˆæ­£å‰‡åŒ–å¼·åº¦ï¼‰
* **è©•ä¼°**ï¼š5-fold CV çš„ **MSE**ï¼ˆ`sklearn` å›å‚³è² å€¼ï¼Œæ•…å–è² è™Ÿï¼‰

```python
@st.cache_resource
def optimize_with_optuna(X_tr, y_tr, n_trials_param):
    study = optuna.create_study(direction='minimize', study_name='lasso_optimization')
    study.optimize(lambda trial: objective(trial, X_tr, y_tr), n_trials=n_trials_param, show_progress_bar=False)
    return study
```

* **è³‡æºå‹å¿«å–**ï¼šé¿å…æ¯æ¬¡äº’å‹•éƒ½é‡è·‘è©¦é©—
* **ç›®æ¨™**ï¼šæœ€å°åŒ– CV-MSE

---

## 6) æ¨¡å‹è¨“ç·´ï¼ˆLassoï¼‰

```python
@st.cache_resource
def train_lasso_model(X_tr, y_tr, alpha_param):
    model = Lasso(alpha=alpha_param, random_state=42, max_iter=10000)
    model.fit(X_tr, y_tr)
    return model
```

* æŒ‡å®š `max_iter=10000` ä»¥ç¢ºä¿æ”¶æ–‚ï¼›å›å‚³å·²æ“¬åˆçš„æ¨¡å‹ã€‚

---

## 7) çµ±ä¸€è©•ä¼°å‡½å¼ï¼ˆè¨“ç·´/æ¸¬è©¦ï¼‹é æ¸¬å›å‚³ï¼‰

```python
def evaluate_model(model, X_tr, X_te, y_tr, y_te):
    y_train_pred = model.predict(X_tr)
    y_test_pred = model.predict(X_te)
  
    return {
        'train': {
            'mse': mean_squared_error(y_tr, y_train_pred),
            'mae': mean_absolute_error(y_tr, y_train_pred),
            'r2': r2_score(y_tr, y_train_pred)
        },
        'test': {
            'mse': mean_squared_error(y_te, y_test_pred),
            'mae': mean_absolute_error(y_te, y_test_pred),
            'r2': r2_score(y_te, y_test_pred)
        },
        'predictions': {
            'train': (y_tr, y_train_pred),
            'test': (y_te, y_test_pred)
        }
    }
```

* åŒæ™‚è¨ˆç®— **MSEï¼MAEï¼RÂ²**ï¼Œä¸¦å›å‚³å¯¦éš›èˆ‡é æ¸¬å€¼ä¾›å¾ŒçºŒè¦–è¦ºåŒ–ä½¿ç”¨ã€‚

---

## 8) å¤šåˆ†é ï¼ˆTabsï¼‰â€” å°æ‡‰ CRISP-DM

```python
tab1, tab2, tab3, tab4, tab5, tab6 = st.tabs([
    "ğŸ“Š Step 1: è³‡æ–™è¼‰å…¥", 
    "âš™ï¸ Step 2: è³‡æ–™é è™•ç†",
    "ğŸ¤– Step 3: æ¨¡å‹å»ºç«‹",
    "ğŸ“ˆ Step 4: æ¨¡å‹è©•ä¼°",
    "âœ¨ Step 4+: Optuna å„ªåŒ–",
    "ğŸš€ Step 5: æ¨¡å‹éƒ¨ç½²"
])
```

* å‘ˆç¾å®Œæ•´æµç¨‹å°è¦½ï¼Œè©•é‡èˆ‡å„ªåŒ–æ‹†é é¡¯ç¤ºã€‚

---

## 9) Tab1ï¼šè³‡æ–™ç†è§£ï¼ˆçµ±è¨ˆï¼‹åˆ†ä½ˆï¼‰

**é—œéµç‰‡æ®µï¼š**

```python
st.success(f"âœ… æˆåŠŸè¼‰å…¥è³‡æ–™ï¼æª”æ¡ˆè·¯å¾‘: {PATH}")
st.dataframe(df.head(15), use_container_width=True)
st.dataframe(df.describe(), use_container_width=True)

fig, ax = plt.subplots(figsize=(10, 4))
ax.hist(df['Weight'], bins=20, color='skyblue', edgecolor='black', alpha=0.7)
st.pyplot(fig)
```

* é¡¯ç¤ºè³‡æ–™é è¦½ã€æè¿°çµ±è¨ˆã€**ç›®æ¨™è®Šæ•¸åˆ†ä½ˆ**ï¼ˆæª¢æŸ¥åæ…‹èˆ‡æ¥µç«¯å€¼ï¼‰ã€‚

---

## 10) Tab2ï¼šè³‡æ–™æº–å‚™ï¼ˆåˆ‡åˆ†ã€ç‰¹å¾µã€ç›¸é—œæ€§ï¼‰

**é—œéµç‰‡æ®µï¼š**

```python
st.metric("è¨“ç·´é›†æ¨£æœ¬æ•¸", len(X_train))
st.metric("æ¸¬è©¦é›†æ¨£æœ¬æ•¸", len(X_test))
st.write(list(feature_names))

X_df = pd.DataFrame(X_train, columns=feature_names)
X_df['Weight'] = y_train.values
sns.heatmap(X_df.corr(), annot=True, fmt='.2f', cmap='coolwarm', center=0, ax=ax)
```

* é¡¯ç¤ºåˆ‡åˆ†å¾Œæ•¸é‡èˆ‡**ç·¨ç¢¼å¾Œç‰¹å¾µæ¸…å–®**ã€‚
* ä»¥è¨“ç·´é›†ï¼ˆå·²æ¨™æº–åŒ–å€¼ï¼‰ä¼°ç®—ç›¸é—œæ€§ï¼ˆåŠ å…¥ `Weight` æª¢è¦–é—œè¯æ–¹å‘/å¼·åº¦ï¼‰ã€‚

---

## 11) Tab3ï¼šå»ºæ¨¡ï¼ˆæ‰‹å‹• Î± vs Optunaï¼‰

**é—œéµç‰‡æ®µï¼ˆOptuna åˆ†æ”¯ï¼‰**

```python
study = optimize_with_optuna(X_train, y_train, n_trials)
best_alpha = study.best_params['alpha']
model = train_lasso_model(X_train, y_train, best_alpha)
current_alpha = best_alpha
```

**é—œéµç‰‡æ®µï¼ˆæ‰‹å‹•åˆ†æ”¯ï¼‰**

```python
model = train_lasso_model(X_train, y_train, alpha)
current_alpha = alpha
```

**ä¿‚æ•¸è¦–è¦ºåŒ–èˆ‡ç¨€ç–åº¦**

```python
coef_df = pd.DataFrame({
    'ç‰¹å¾µåç¨±': feature_names,
    'ä¿‚æ•¸': model.coef_,
    'çµ•å°å€¼': np.abs(model.coef_)
}).sort_values('çµ•å°å€¼', ascending=False)

colors = ['green' if x != 0 else 'lightgray' for x in coef_df['ä¿‚æ•¸']]
ax.barh(coef_df['ç‰¹å¾µåç¨±'], coef_df['ä¿‚æ•¸'], color=colors, edgecolor='black')
non_zero = np.sum(model.coef_ != 0)
```

* **Lasso ç‰¹æ€§**ï¼šä»¥ L1 æ­£å‰‡åŒ–å¯¦ç¾**ç‰¹å¾µé¸æ“‡**ï¼ˆä¿‚æ•¸æ¨ç‚º 0ï¼‰ã€‚
* åœ–ä¸Šä»¥ç°è‰²çªå‡ºè¢«ã€Œå£“æˆ 0ã€çš„ç‰¹å¾µï¼Œç¶ è‰²ç‚ºä¿ç•™ç‰¹å¾µã€‚

---

## 12) Tab4ï¼šæ¨¡å‹è©•ä¼°èˆ‡è¨ºæ–·

**æ•´é«”æŒ‡æ¨™**

```python
metrics = evaluate_model(model, X_train, X_test, y_train, y_test)
st.metric("MSE", f"{metrics['test']['mse']:.2f}")
st.metric("MAE", f"{metrics['test']['mae']:.2f}")
st.metric("RÂ²", f"{metrics['test']['r2']:.4f}")
```

**é/æ¬ æ“¬åˆåµæ¸¬**

```python
train_r2 = metrics['train']['r2']; test_r2 = metrics['test']['r2']
r2_diff = train_r2 - test_r2

if r2_diff > 0.15 and train_r2 > 0.7:
    st.warning("éæ“¬åˆ")
elif train_r2 < 0.5 and test_r2 < 0.5:
    st.warning("æ¬ æ“¬åˆ")
else:
    st.success("æ¨¡å‹è¡¨ç¾è‰¯å¥½")
```

* **åˆ¤æº–ç›´è§€**ï¼š
  * è¨“ç·´å¾ˆå¥½ä½†æ¸¬è©¦æ˜é¡¯å·® â†’ éæ“¬åˆ
  * å…©é‚Šéƒ½å·® â†’ æ¬ æ“¬åˆ
  * å…©é‚Šéƒ½ä¸éŒ¯ã€å·®è·å° â†’ åˆç†æ“¬åˆ

**é æ¸¬å°è§’ç·šæ•£ä½ˆåœ–ï¼‹æ®˜å·®åœ–**

```python
# y_true vs y_predï¼›45Â° å°è§’ç·šç†æƒ³
ax1.scatter(y_tr, y_train_pred)
ax1.plot([y_tr.min(), y_tr.max()], [y_tr.min(), y_tr.max()], 'r--')

# æ®˜å·® vs é æ¸¬ï¼›æª¢æŸ¥æ˜¯å¦æœ‰çµæ§‹æ€§åèª¤ï¼ˆééš¨æ©Ÿï¼‰
ax2.scatter(y_test_pred, test_residuals)
ax2.axhline(y=0, color='r', linestyle='--')
```

---

## 13) Tab5ï¼šOptuna è¦–è¦ºåŒ–èˆ‡æˆæ•ˆå°æ¯”

**æœ€ä½³è§£æ‘˜è¦èˆ‡æ­·å²**

```python
st.metric("æœ€ä½³ Alpha", f"{study.best_params['alpha']:.6f}")
st.metric("æœ€ä½³ MSE", f"{study.best_value:.2f}")

fig = plot_optimization_history(study)  # è‹¥å¤±æ•—å‰‡è‡ªç¹ªæ›²ç·š
```

**åƒæ•¸åˆ†ä½ˆèˆ‡è¡¨ç¾**

```python
trials_df = study.trials_dataframe()[['number','value','params_alpha','state']]
ax.scatter(trials_df['Alpha'], trials_df['MSE'])
ax.set_xscale('log')
```

* æª¢è¦– Î±ï¼ˆlog è»¸ï¼‰èˆ‡ MSE çš„é—œä¿‚ï¼Œæ¨™å‡ºæœ€ä½³ Î±ã€‚

**èˆ‡é è¨­ Î±=1.0 å°æ¯”**

```python
default_model = train_lasso_model(X_train, y_train, 1.0)
default_metrics = evaluate_model(default_model, X_train, X_test, y_train, y_test)

improvement_r2  = ((metrics['test']['r2'] - default_metrics['test']['r2']) / abs(default_metrics['test']['r2']) * 100)
improvement_mse = ((default_metrics['test']['mse'] - metrics['test']['mse']) / default_metrics['test']['mse'] * 100)
```

* æä¾›**æ•ˆèƒ½æ”¹å–„ç™¾åˆ†æ¯”**ï¼Œé‡åŒ– Optuna å¸¶ä¾†çš„å¥½è™•ã€‚

---

## 14) Tab6ï¼šéƒ¨ç½²ï¼ˆå³æ™‚ï¼æ‰¹æ¬¡æ¨è«–ï¼‰

**å³æ™‚æ¨è«– UI èˆ‡å‰è™•ç†**

```python
# ä»¥åŸè¨“ç·´ç‰¹å¾µé †åºå°é½Šï¼Œç¼ºå°‘çš„ dummy æ¬„è£œ 0
input_encoded = pd.get_dummies(input_data, columns=['Species'], drop_first=True)
for col in feature_names:
    if col not in input_encoded.columns:
        input_encoded[col] = 0
input_encoded = input_encoded[feature_names]

input_scaled = scaler.transform(input_encoded)
prediction = model.predict(input_scaled)[0]
```

* **é—œéµ**ï¼š**æ¨è«–æ™‚çš„ç‰¹å¾µæ¬„ä½é †åº**å¿…é ˆèˆ‡è¨“ç·´ä¸€è‡´ï¼›æ²’å‡ºç¾çš„é¡åˆ¥ dummy æ¬„è£œ 0ã€‚
* ä»¥è¨“ç·´æ™‚ `scaler` é€²è¡Œç›¸åŒæ¨™æº–åŒ–ï¼Œå† `model.predict`ã€‚

**ä»¥ MAE ç²—ä¼° 95% ä¿¡è³´å€é–“ï¼ˆè¿‘ä¼¼ï¼‰**

```python
confidence_interval = metrics['test']['mae'] * 1.96
```

* å¯¦å‹™ä¸Š MAE ä¸¦éæ¨™æº–å·®ï¼Œä½†å¯ä½œç‚º**ç²—ç•¥**ä¸ç¢ºå®šæ€§ç¯„åœç¤ºæ„ã€‚

**æ‰¹æ¬¡æ¨è«–èˆ‡ä¸‹è¼‰**

```python
uploaded_file = st.file_uploader("ä¸Šå‚³ CSV æª”æ¡ˆ", type=['csv'])
# ... åŒæ­¥ä¸€æ¨£çš„ç·¨ç¢¼/æ¨™æº–åŒ–/é æ¸¬ ...
st.download_button("ğŸ“¥ ä¸‹è¼‰çµæœ", csv, "predictions.csv", "text/csv", type="primary")
```

---

## 15) é å°¾ç¸½çµèˆ‡è¦–è¦ºæ•ˆæœ

```python
st.balloons()
st.success("ğŸ‰ å°ˆæ¡ˆå®Œæˆï¼æ„Ÿè¬ä½¿ç”¨æœ¬ç³»çµ±ï¼")
```

* å®Œæˆå‹•ç•«èˆ‡æ‘˜è¦æŒ‡æ¨™ï¼ˆé¡¯ç¤º Î±ã€RÂ²ã€MAEã€æœ€é‡è¦ç‰¹å¾µï¼‰ã€‚

---

# è©•ä¼°æŒ‡æ¨™æ„ç¾©ï¼ˆå¿«é€Ÿè¤‡ç¿’ï¼‰

* **MSE**ï¼ˆå‡æ–¹èª¤å·®ï¼‰ï¼šæ‡²ç½°å¤§èª¤å·®æ›´é‡ï¼›å°é›¢ç¾¤å€¼æ•æ„Ÿã€‚
* **MAE**ï¼ˆå¹³å‡çµ•å°èª¤å·®ï¼‰ï¼šèª¤å·®çš„å¹³å‡çµ•å°å€¼ï¼Œæ˜“è§£é‡‹ã€æŠ—é›¢ç¾¤ã€‚
* **RÂ²**ï¼ˆæ±ºå®šä¿‚æ•¸ï¼‰ï¼šè§£é‡‹è®Šç•°æ¯”ä¾‹ï¼Œè¶Šæ¥è¿‘ 1 è¶Šå¥½ï¼›ä½æ–¼ 0 ä»£è¡¨æ¯”å¸¸æ•¸æ¨¡å‹é‚„å·®ã€‚
* **Overfitï¼Underfit åˆ¤æ–·**ï¼š
  * **Overfit**ï¼šè¨“ç·´ RÂ² é«˜ã€æ¸¬è©¦ RÂ² é¡¯è‘—ä½ï¼ˆå·®è·å¤§ï¼‰ï¼›
  * **Underfit**ï¼šè¨“ç·´ã€æ¸¬è©¦ RÂ² éƒ½ä½ï¼›
  * **Balanced**ï¼šå…©è€…çš†é«˜ä¸”å·®è·å°ã€‚

---

# å¸¸è¦‹éŒ¯èª¤èˆ‡ä¿®æ­£å»ºè­°ï¼ˆå‹™å¿…æª¢æŸ¥ï¼‰

1. **è³‡æ–™è·¯å¾‘éŒ¯èª¤**

```python
# åŸæœ¬
PATH = "./dataset/Fishers maket.csv"

# å»ºè­°ï¼ˆå…¸å‹æª”åï¼‰
PATH = "./dataset/Fish.csv"
```

2. **é¡åˆ¥æ¬„ä½å°é½Š**ï¼šéƒ¨ç½²ï¼ˆå³æ™‚/æ‰¹æ¬¡ï¼‰æ™‚ä¸€å®šè¦

```python
# é€æ¬„è£œé½Šç¼ºå¤± dummy
for col in feature_names:
    if col not in X_batch_encoded.columns:
        X_batch_encoded[col] = 0
X_batch_encoded = X_batch_encoded[feature_names]
```

3. **å¿«å–å¤±æ•ˆ**ï¼ˆç•¶ä½ æ”¹äº†è³‡æ–™è·¯å¾‘æˆ–åƒæ•¸ï¼‰

* è‹¥é‡åˆ°å…§å®¹ä¸æ›´æ–°ï¼Œè«‹åœ¨ Streamlit ä»‹é¢é»æ“Š **Rerun** æˆ–æ¸…å¿«å–ã€‚
* `@st.cache_data` é‡å°è³‡æ–™ã€`@st.cache_resource` é‡å°æ¨¡å‹/Study ç‰©ä»¶ã€‚

4. **Optuna åœ–å½¢ç›¸ä¾**

* `plot_optimization_history` å¤±æ•—æ™‚ä½ å·²æä¾›å¾Œå‚™æ‰‹ç¹ªæŠ˜ç·šï¼Œé€™å¾ˆå¯¦ç”¨ã€‚
* è‹¥è¦å†åŠ ã€Œåƒæ•¸é‡è¦åº¦ã€ï¼Œå¯è£œï¼š
  ```python
  try:
      fig_imp = plot_param_importances(study)
      st.pyplot(fig_imp)
  except:
      pass
  ```

5. **æ”¶æ–‚å•é¡Œ**

* è‹¥å‡ºç¾ Lasso æœªæ”¶æ–‚ï¼Œå¯å†èª¿é«˜ `max_iter` æˆ–èª¿æ•´ `alpha` ç¯„åœã€‚

---

# å¯å»¶ä¼¸æ”¹é€²ï¼ˆé¸è®€ï¼‰

* **äº¤å‰é©—è­‰åˆ‡åˆ†æ³•**ï¼šä»¥ `KFold`/`GroupKFold` æå‡ä¼°è¨ˆç©©å®šæ€§ã€‚
* **ç‰¹å¾µå·¥ç¨‹**ï¼šé•·åº¦ã€å¯¬åº¦ã€èº«é«˜ç­‰æ¬„ä½äº¤äº’é …ï¼ˆä¾‹å¦‚ `LengthÃ—Width`ï¼‰ï¼Œæˆ–å°é«˜åº¦åšå°æ•¸è®Šæ›ã€‚
* **ç©©å¥è©•ä¼°**ï¼šåŠ å…¥ `MedAE`ï¼ˆä¸­ä½æ•¸çµ•å°èª¤å·®ï¼‰ã€`RMSE`ï¼ˆæ ¹è™Ÿ MSEï¼‰ã€‚
* **æ¯”è¼ƒåŸºæº–**ï¼šåŠ å…¥ Ridgeï¼ElasticNetï¼RandomForest å°ç…§ã€‚
* **æ¨¡å‹è§£é‡‹**ï¼šä»¥ `Permutation Importance` æˆ– SHAPï¼ˆå›æ­¸ï¼‰è£œå……é‡è¦åº¦ã€‚

---

## å°æŠ„ï¼šæœ¬ç³»çµ±èˆ‡ CRISP-DM çš„å°é½Šï¼ˆä¸€å¥è©±ç‰ˆæœ¬ï¼‰

* **ç†è§£**ï¼ˆTab1ï¼‰ï¼šçœ‹è³‡æ–™ã€çœ‹ Weight åˆ†ä½ˆï¼›
* **æº–å‚™**ï¼ˆTab2ï¼‰ï¼šOne-hotï¼‹Splitï¼‹Scaleï¼‹çœ‹ç›¸é—œæ€§ï¼›
* **å»ºæ¨¡**ï¼ˆTab3ï¼‰ï¼šLassoï¼ˆæ‰‹å‹•/Optunaï¼‰ï¼‹ä¿‚æ•¸ç¨€ç–ï¼›
* **è©•ä¼°**ï¼ˆTab4/5ï¼‰ï¼šRÂ²/MSE/MAEã€éæ“¬åˆæª¢æŸ¥ã€æ®˜å·®ã€Optuna è¦–è¦ºåŒ–èˆ‡å°æ¯”ï¼›
* **éƒ¨ç½²**ï¼ˆTab6ï¼‰ï¼šæ¬„ä½å°é½Šâ†’æ¨™æº–åŒ–â†’æ¨è«–â†’ä¸‹è¼‰ã€‚

---

> ä»¥ä¸Š Markdown å¯ç›´æ¥ä½œç‚ºä½œæ¥­å ±å‘Šæäº¤ï¼›è‹¥éœ€è¦æˆ‘æŠŠå®ƒ**æ’ç‰ˆæˆæ›´æ­£å¼çš„æ•™å­¸è¬›ç¾©**ï¼ˆå«åœ–æ¨™ã€ç« ç¯€ç·¨è™Ÿã€ç¸®æ’è¡¨æ ¼ï¼‰ï¼Œæˆ–**æ”¹å¯«ç‚ºä¸­è‹±é›™èªç‰ˆ**ï¼Œå‘Šè¨´æˆ‘ä½ åå¥½çš„æ ¼å¼å³å¯ã€‚
>
